{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nibabel\n",
    "# !pip install medpy\n",
    "# !pip install tensorboard\n",
    "# !pip install --upgrade tensorboard\n",
    "# !pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "from medpy.io import load\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpl_image_grid(images):\n",
    "    \"\"\"\n",
    "    Create an image grid from an array of images. Show up to 16 images in one figure\n",
    "\n",
    "    Arguments:\n",
    "        image {Torch tensor} -- NxWxH array of images\n",
    "\n",
    "    Returns:\n",
    "        Matplotlib figure\n",
    "    \"\"\"\n",
    "    # Create a figure to contain the plot.\n",
    "    n = min(images.shape[0], 16) # no more than 16 thumbnails\n",
    "    rows = 4\n",
    "    cols = (n // 4) + (1 if (n % 4) != 0 else 0)\n",
    "    figure = plt.figure(figsize=(2*rows, 2*cols))\n",
    "    plt.subplots_adjust(0, 0, 1, 1, 0.001, 0.001)\n",
    "    for i in range(n):\n",
    "        # Start next subplot.\n",
    "        plt.subplot(cols, rows, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        if images.shape[1] == 3:\n",
    "            # this is specifically for 3 softmax'd classes with 0 being bg\n",
    "            # We are building a probability map from our three classes using \n",
    "            # fractional probabilities contained in the mask\n",
    "            vol = images[i].detach().numpy()\n",
    "            img = [[[(1-vol[0,x,y])*vol[1,x,y], (1-vol[0,x,y])*vol[2,x,y], 0] \\\n",
    "                            for y in range(vol.shape[2])] \\\n",
    "                            for x in range(vol.shape[1])]\n",
    "            plt.imshow(img)\n",
    "        else: # plotting only 1st channel\n",
    "            plt.imshow((images[i, 0]*255).int(), cmap= \"gray\")\n",
    "\n",
    "    return figure\n",
    "\n",
    "def log_to_tensorboard(writer, loss, data, target, prediction_softmax, prediction, counter):\n",
    "    \"\"\"Logs data to Tensorboard\n",
    "\n",
    "    Arguments:\n",
    "        writer {SummaryWriter} -- PyTorch Tensorboard wrapper to use for logging\n",
    "        loss {float} -- loss\n",
    "        data {tensor} -- image data\n",
    "        target {tensor} -- ground truth label\n",
    "        prediction_softmax {tensor} -- softmax'd prediction\n",
    "        prediction {tensor} -- raw prediction (to be used in argmax)\n",
    "        counter {int} -- batch and epoch counter\n",
    "    \"\"\"\n",
    "    writer.add_scalar(\"Loss\",\\\n",
    "                    loss, counter)\n",
    "    writer.add_figure(\"Image Data\",\\\n",
    "        mpl_image_grid(data.float().cpu()), global_step=counter)\n",
    "    writer.add_figure(\"Mask\",\\\n",
    "        mpl_image_grid(target.float().cpu()), global_step=counter)\n",
    "    writer.add_figure(\"Probability map\",\\\n",
    "        mpl_image_grid(prediction_softmax.cpu()), global_step=counter)\n",
    "    writer.add_figure(\"Prediction\",\\\n",
    "        mpl_image_grid(torch.argmax(prediction.cpu(), dim=1, keepdim=True)), global_step=counter)\n",
    "\n",
    "def save_numpy_as_image(arr, path):\n",
    "    \"\"\"\n",
    "    This saves image (2D array) as a file using matplotlib\n",
    "\n",
    "    Arguments:\n",
    "        arr {array} -- 2D array of pixels\n",
    "        path {string} -- path to file\n",
    "    \"\"\"\n",
    "    plt.imshow(arr, cmap=\"gray\") #Needs to be in row,col order\n",
    "    plt.savefig(path)\n",
    "\n",
    "def med_reshape(image, new_shape):\n",
    "    \"\"\"\n",
    "    This function reshapes 3D data to new dimension padding with zeros\n",
    "    and leaving the content in the top-left corner\n",
    "\n",
    "    Arguments:\n",
    "        image {array} -- 3D array of pixel data\n",
    "        new_shape {3-tuple} -- expected output shape\n",
    "\n",
    "    Returns:\n",
    "        3D array of desired shape, padded with zeroes\n",
    "    \"\"\"\n",
    "\n",
    "    reshaped_image = np.zeros(new_shape)\n",
    "    x, y, z = image.shape\n",
    "    reshaped_image[:x, :y, :z] = image\n",
    "\n",
    "    return reshaped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## volume_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dice3d(a, b):\n",
    "    \"\"\"\n",
    "    This will compute the Dice Similarity coefficient for two 3-dimensional volumes\n",
    "    Volumes are expected to be of the same size. We are expecting binary masks -\n",
    "    0's are treated as background and anything else is counted as data\n",
    "\n",
    "    Arguments:\n",
    "        a {Numpy array} -- 3D array with first volume\n",
    "        b {Numpy array} -- 3D array with second volume\n",
    "\n",
    "    Returns:\n",
    "        float\n",
    "    \"\"\"\n",
    "    if len(a.shape) != 3 or len(b.shape) != 3:\n",
    "        raise Exception(f\"Expecting 3 dimensional inputs, got {a.shape} and {b.shape}\")\n",
    "\n",
    "    if a.shape != b.shape:\n",
    "        raise Exception(f\"Expecting inputs of the same shape, got {a.shape} and {b.shape}\")\n",
    "\n",
    "    intersection = np.sum((a>0) * (b>0))\n",
    "    volumes = np.sum(a>0) + np.sum(b>0)\n",
    "\n",
    "    if volumes == 0:\n",
    "        return -1\n",
    "\n",
    "    return 2.*float(intersection) / float(volumes)\n",
    "\n",
    "def Jaccard3d(a, b):\n",
    "    \"\"\"\n",
    "    This will compute the Jaccard Similarity coefficient for two 3-dimensional volumes\n",
    "    Volumes are expected to be of the same size. We are expecting binary masks - \n",
    "    0's are treated as background and anything else is counted as data\n",
    "\n",
    "    Arguments:\n",
    "        a {Numpy array} -- 3D array with first volume\n",
    "        b {Numpy array} -- 3D array with second volume\n",
    "\n",
    "    Returns:\n",
    "        float\n",
    "    \"\"\"\n",
    "    if len(a.shape) != 3 or len(b.shape) != 3:\n",
    "        raise Exception(f\"Expecting 3 dimensional inputs, got {a.shape} and {b.shape}\")\n",
    "\n",
    "    if a.shape != b.shape:\n",
    "        raise Exception(f\"Expecting inputs of the same shape, got {a.shape} and {b.shape}\")\n",
    "        \n",
    "    intersection = np.sum((a>0) * (b>0))\n",
    "    union = np.sum(((a>0) + (b>0))>0)\n",
    "    \n",
    "    if union == 0:\n",
    "        return -1\n",
    "    \n",
    "    return float(intersection) / float(union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HippocampusDatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadHippocampusData(root_dir, y_shape, z_shape):\n",
    "    '''\n",
    "    This function loads our dataset form disk into memory,\n",
    "    reshaping output to common size\n",
    "\n",
    "    Arguments:\n",
    "        volume {Numpy array} -- 3D array representing the volume\n",
    "\n",
    "    Returns:\n",
    "        Array of dictionaries with data stored in seg and image fields as \n",
    "        Numpy arrays of shape [AXIAL_WIDTH, Y_SHAPE, Z_SHAPE]\n",
    "    '''\n",
    "\n",
    "    image_dir = join(root_dir, 'images')\n",
    "    label_dir = join(root_dir, 'labels')\n",
    "\n",
    "    images = [f for f in listdir(image_dir) if (\n",
    "        isfile(join(image_dir, f)) and f[0] != \".\")]\n",
    "\n",
    "    out = []\n",
    "    for f in images:\n",
    "\n",
    "        # We would benefit from mmap load method here if dataset doesn't fit into memory\n",
    "        # Images are loaded here using MedPy's load method. We will ignore header \n",
    "        # since we will not use it\n",
    "        image, _ = load(join(image_dir, f))\n",
    "        label, _ = load(join(label_dir, f))\n",
    "\n",
    "        # normalize all images (but not labels) so that values are in [0..1] range\n",
    "        image = image / np.max(image)\n",
    "\n",
    "        # reshape data since CNN tensors need to be of the same size.\n",
    "        # since we feed individual slices to the CNN, we only need to extend 2 dimensions (coronal and sagittal) out of 3 \n",
    "\n",
    "        image = med_reshape(image, new_shape=(image.shape[0], y_shape, z_shape))\n",
    "        label = med_reshape(label, new_shape=(label.shape[0], y_shape, z_shape)).astype(int)\n",
    "\n",
    "        # Why do we need to cast label to int?\n",
    "        # To get distinct class of labels \n",
    "\n",
    "        out.append({\"image\": image, \"seg\": label, \"filename\": f})\n",
    "\n",
    "    # Hippocampus dataset only takes about 300 Mb RAM, so we can afford to keep it all in RAM\n",
    "    print(f\"Processed {len(out)} files, total {sum([x['image'].shape[0] for x in out])} slices\")\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SlicesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class represents an indexable Torch dataset\n",
    "    which could be consumed by the PyTorch DataLoader class\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "        self.slices = []\n",
    "\n",
    "        for i, d in enumerate(data):\n",
    "            for j in range(d[\"image\"].shape[0]):\n",
    "                self.slices.append((i, j))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This method is called by PyTorch DataLoader class to return a sample with id idx\n",
    "\n",
    "        Arguments: \n",
    "            idx {int} -- id of sample\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of 2 Torch Tensors of dimensions [1, W, H]\n",
    "        \"\"\"\n",
    "        slc = self.slices[idx]\n",
    "        sample = dict()\n",
    "        sample[\"id\"] = idx\n",
    "        \n",
    "        # create two new keys in the \"sample\" dictionary, \"image\" and \"seg\"\n",
    "        i, j = slc\n",
    "        sample['image'] = torch.from_numpy(self.data[i]['image'][j, :, :][None, :])\n",
    "        sample['seg'] = torch.from_numpy(self.data[i]['seg'][j, :, :][None, :])\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This method is called by PyTorch DataLoader class to return number of samples in the dataset\n",
    "\n",
    "        Returns:\n",
    "            int\n",
    "        \"\"\"\n",
    "        return len(self.slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecursiveUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=3, in_channels=1, initial_filter_size=64, kernel_size=3, num_downs=4, norm_layer=nn.InstanceNorm2d):\n",
    "        # norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(in_channels=initial_filter_size * 2 ** (num_downs-1), out_channels=initial_filter_size * 2 ** num_downs,\n",
    "                                             num_classes=num_classes, kernel_size=kernel_size, norm_layer=norm_layer, innermost=True)\n",
    "        for i in range(1, num_downs):\n",
    "            unet_block = UnetSkipConnectionBlock(in_channels=initial_filter_size * 2 ** (num_downs-(i+1)),\n",
    "                                                 out_channels=initial_filter_size * 2 ** (num_downs-i),\n",
    "                                                 num_classes=num_classes, kernel_size=kernel_size, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(in_channels=in_channels, out_channels=initial_filter_size,\n",
    "                                             num_classes=num_classes, kernel_size=kernel_size, submodule=unet_block, norm_layer=norm_layer,\n",
    "                                             outermost=True)\n",
    "\n",
    "        self.model = unet_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Defines the submodule with skip connection.\n",
    "# X -------------------identity---------------------- X\n",
    "#   |-- downsampling -- |submodule| -- upsampling --|\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None, num_classes=1, kernel_size=3,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.InstanceNorm2d, use_dropout=False):\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        # downconv\n",
    "        pool = nn.MaxPool2d(2, stride=2)\n",
    "        conv1 = self.contract(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, norm_layer=norm_layer)\n",
    "        conv2 = self.contract(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, norm_layer=norm_layer)\n",
    "\n",
    "        # upconv\n",
    "        conv3 = self.expand(in_channels=out_channels*2, out_channels=out_channels, kernel_size=kernel_size)\n",
    "        conv4 = self.expand(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
    "\n",
    "        if outermost:\n",
    "            final = nn.Conv2d(out_channels, num_classes, kernel_size=1)\n",
    "            down = [conv1, conv2]\n",
    "            up = [conv3, conv4, final]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(in_channels*2, in_channels,\n",
    "                                        kernel_size=2, stride=2)\n",
    "            model = [pool, conv1, conv2, upconv]\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(in_channels*2, in_channels, kernel_size=2, stride=2)\n",
    "\n",
    "            down = [pool, conv1, conv2]\n",
    "            up = [conv3, conv4, upconv]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    @staticmethod\n",
    "    def contract(in_channels, out_channels, kernel_size=3, norm_layer=nn.InstanceNorm2d):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
    "            norm_layer(out_channels),\n",
    "            nn.LeakyReLU(inplace=True))\n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def expand(in_channels, out_channels, kernel_size=3):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def center_crop(layer, target_width, target_height):\n",
    "        batch_size, n_channels, layer_width, layer_height = layer.size()\n",
    "        xy1 = (layer_width - target_width) // 2\n",
    "        xy2 = (layer_height - target_height) // 2\n",
    "        return layer[:, :, xy1:(xy1 + target_width), xy2:(xy2 + target_height)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            crop = self.center_crop(self.model(x), x.size()[2], x.size()[3])\n",
    "            return torch.cat([x, crop], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNetInferenceAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetInferenceAgent:\n",
    "    \"\"\"\n",
    "    Stores model and parameters and some methods to handle inferencing\n",
    "    \"\"\"\n",
    "    def __init__(self, parameter_file_path='', model=None, device=\"cpu\", patch_size=64):\n",
    "\n",
    "        self.model = model\n",
    "        self.patch_size = patch_size\n",
    "        self.device = device\n",
    "\n",
    "        if model is None:\n",
    "            self.model = UNet(num_classes=3)\n",
    "\n",
    "        if parameter_file_path:\n",
    "            self.model.load_state_dict(torch.load(parameter_file_path, map_location=self.device))\n",
    "\n",
    "        self.model.to(device)\n",
    "\n",
    "    def single_volume_inference(self, volume):\n",
    "        \"\"\"\n",
    "        Runs inference on a single volume of conformant patch size\n",
    "\n",
    "        Arguments:\n",
    "            volume {Numpy array} -- 3D array representing the volume\n",
    "\n",
    "        Returns:\n",
    "            3D NumPy array with prediction mask\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # volume is a numpy array of shape [X,Y,Z] and I will slice X axis\n",
    "        slices = []\n",
    "\n",
    "        # create mask for each slice across the X (0th) dimension. \n",
    "        # put all slices into a 3D Numpy array\n",
    "\n",
    "        for ix in range(0, volume.shape[0]):\n",
    "#             img = volume[ix,:,:]\n",
    "#             slc = img.astype(np.single)/np.max(img)\n",
    "#             slc_tensor = torch.from_numpy(slc).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "            slice_tensor = torch.from_numpy(volume[ix,:,:].astype(np.single)).unsqueeze(0).unsqueeze(0)\n",
    "            pred = self.model(slice_tensor.to(self.device))\n",
    "            mask = torch.argmax(np.squeeze(pred.cpu().detach()), dim=0)\n",
    "            slices.append(mask)\n",
    "        return np.dstack(slices).transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "    \n",
    "    def single_volume_inference_unpadded(self, volume, patch_size):\n",
    "        \"\"\"\n",
    "        Runs inference on a single volume of arbitrary patch size,\n",
    "        padding it to the conformant size first\n",
    "\n",
    "        Arguments:\n",
    "            volume {Numpy array} -- 3D array representing the volume\n",
    "\n",
    "        Returns:\n",
    "            3D NumPy array with prediction mask\n",
    "        \"\"\"\n",
    "        \n",
    "        volume = med_reshape(volume, (volume.shape[0], patch_size, patch_size))\n",
    "        \n",
    "        return single_volume_inference(self, volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNetExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetExperiment:\n",
    "    \"\"\"\n",
    "    This class implements the basic life cycle for a segmentation task with UNet(https://arxiv.org/abs/1505.04597).\n",
    "    The basic life cycle of a UNetExperiment is:\n",
    "\n",
    "        run():\n",
    "            for epoch in n_epochs:\n",
    "                train()\n",
    "                validate()\n",
    "        test()\n",
    "    \"\"\"\n",
    "    def __init__(self, config, split, dataset):\n",
    "        self.n_epochs = config.n_epochs\n",
    "        self.split = split\n",
    "        self._time_start = \"\"\n",
    "        self._time_end = \"\"\n",
    "        self.epoch = 0\n",
    "        self.name = config.name\n",
    "\n",
    "        # Create output folders\n",
    "        dirname = f'{time.strftime(\"%Y-%m-%d_%H%M\", time.gmtime())}_{self.name}'\n",
    "        self.out_dir = os.path.join(config.test_results_dir, dirname)\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "\n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(SlicesDataset(dataset[split[\"train\"]]),\n",
    "                batch_size=config.batch_size, shuffle=True, num_workers=0)\n",
    "        self.val_loader = DataLoader(SlicesDataset(dataset[split[\"val\"]]),\n",
    "                batch_size=config.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        # access volumes directly for testing\n",
    "        self.test_data = dataset[split[\"test\"]]\n",
    "\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"WARNING: No CUDA device is found. This may take significantly longer!\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # use a recursive UNet model from German Cancer Research Center, Division of Medical Image Computing\n",
    "        self.model = UNet()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # use a standard cross-entropy loss since the model output is essentially\n",
    "        # a tensor with softmax prediction of each pixel's probability of belonging to a certain class\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # use standard SGD method to optimize the weights\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.learning_rate)\n",
    "        \n",
    "        # Scheduler helps to update learning rate automatically\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
    "\n",
    "        # Set up Tensorboard. By default it saves data into runs folder. You need to launch\n",
    "        self.tensorboard_train_writer = SummaryWriter(comment=\"_train\")\n",
    "        self.tensorboard_val_writer = SummaryWriter(comment=\"_val\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        This method is executed once per epoch and takes \n",
    "        care of model weight update cycle\n",
    "        \"\"\"\n",
    "        print(f\"Training epoch {self.epoch}...\")\n",
    "        self.model.train()\n",
    "\n",
    "        # Loop over the minibatches\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Feed data to the model and feed target to the loss function\n",
    "            data = batch['image'].float()\n",
    "            target = batch['seg']\n",
    "            prediction = self.model(data.to(self.device))\n",
    "            prediction_softmax = F.softmax(prediction, dim=1)\n",
    "            loss = self.loss_function(prediction_softmax, target[:, 0, :, :].to(self.device))\n",
    "\n",
    "            # What does each dimension of variable prediction represent?\n",
    "            # batch_size, 3 classes, coronal, axial\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (i % 10) == 0:\n",
    "                # Output to console on every 10th batch\n",
    "                print(f\"\\nEpoch: {self.epoch} Train loss: {loss}, {100*(i+1)/len(self.train_loader):.1f}% complete\")\n",
    "\n",
    "                counter = 100*self.epoch + 100*(i/len(self.train_loader))\n",
    "\n",
    "                log_to_tensorboard(\n",
    "                    self.tensorboard_train_writer,\n",
    "                    loss,\n",
    "                    data,\n",
    "                    target,\n",
    "                    prediction_softmax,\n",
    "                    prediction,\n",
    "                    counter)\n",
    "\n",
    "            print(\".\", end='')\n",
    "\n",
    "        print(\"\\nTraining complete\")\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        This method runs validation cycle, using same metrics as \n",
    "        Train method. Note that model needs to be switched to eval\n",
    "        mode and no_grad needs to be called so that gradients do not \n",
    "        propagate\n",
    "        \"\"\"\n",
    "        print(f\"Validating epoch {self.epoch}...\")\n",
    "\n",
    "        # Turn off gradient accumulation by switching model to \"eval\" mode\n",
    "        self.model.eval()\n",
    "        loss_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(self.val_loader):              \n",
    "                data = batch['image'].float()\n",
    "                target = batch['seg']\n",
    "                prediction = self.model(data.to(self.device))\n",
    "                prediction_softmax = F.softmax(prediction, dim=1)\n",
    "                loss = self.loss_function(prediction_softmax, target[:, 0, :, :].to(self.device))\n",
    "\n",
    "                print(f\"Batch {i}. Data shape {data.shape} Loss {loss}\")\n",
    "\n",
    "                # We report loss that is accumulated across all of validation set\n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "        self.scheduler.step(np.mean(loss_list))\n",
    "\n",
    "        log_to_tensorboard(\n",
    "            self.tensorboard_val_writer,\n",
    "            np.mean(loss_list),\n",
    "            data,\n",
    "            target,\n",
    "            prediction_softmax, \n",
    "            prediction,\n",
    "            (self.epoch+1) * 100)\n",
    "        print(f\"Validation complete\")\n",
    "\n",
    "    def save_model_parameters(self):\n",
    "        \"\"\"\n",
    "        Saves model parameters to a file in results directory\n",
    "        \"\"\"\n",
    "        path = os.path.join(self.out_dir, \"model.pth\")\n",
    "\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_model_parameters(self, path=''):\n",
    "        \"\"\"\n",
    "        Loads model parameters from a supplied path or a\n",
    "        results directory\n",
    "        \"\"\"\n",
    "        if not path:\n",
    "            model_path = os.path.join(self.out_dir, \"model.pth\")\n",
    "        else:\n",
    "            model_path = path\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            self.model.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            raise Exception(f\"Could not find path {model_path}\")\n",
    "\n",
    "    def run_test(self):\n",
    "        \"\"\"\n",
    "        This runs test cycle on the test dataset.\n",
    "        Note that process and evaluations are quite different\n",
    "        Here we are computing a lot more metrics and returning\n",
    "        a dictionary that could later be persisted as JSON\n",
    "        \"\"\"\n",
    "        print(\"Testing...\")\n",
    "        self.model.eval()\n",
    "\n",
    "        inference_agent = UNetInferenceAgent(model=self.model, device=self.device)\n",
    "\n",
    "        out_dict = {}\n",
    "        out_dict[\"volume_stats\"] = []\n",
    "        dc_list = []\n",
    "        jc_list = []\n",
    "\n",
    "        # for every in test set\n",
    "        for i, x in enumerate(self.test_data):\n",
    "            pred_label = inference_agent.single_volume_inference(x[\"image\"])\n",
    "\n",
    "            # We compute and report Dice and Jaccard similarity coefficients which \n",
    "            # assess how close our volumes are to each other\n",
    "\n",
    "            dc = Dice3d(pred_label, x[\"seg\"])\n",
    "            jc = Jaccard3d(pred_label, x[\"seg\"])\n",
    "            dc_list.append(dc)\n",
    "            jc_list.append(jc)\n",
    "\n",
    "            # STAND-OUT SUGGESTION: By way of exercise, consider also outputting:\n",
    "            # * Sensitivity and specificity (and explain semantic meaning in terms of \n",
    "            #   under/over segmenting)\n",
    "            # * Dice-per-slice and render combined slices with lowest and highest DpS\n",
    "            # * Dice per class (anterior/posterior)\n",
    "\n",
    "            out_dict[\"volume_stats\"].append({\n",
    "                \"filename\": x['filename'],\n",
    "                \"dice\": dc,\n",
    "                \"jaccard\": jc\n",
    "                })\n",
    "            print(f\"{x['filename']} Dice {dc:.4f}. {100*(i+1)/len(self.test_data):.2f}% complete\")\n",
    "\n",
    "        out_dict[\"overall\"] = {\n",
    "            \"mean_dice\": np.mean(dc_list),\n",
    "            \"mean_jaccard\": np.mean(jc_list)}\n",
    "\n",
    "        print(\"\\nTesting complete.\")\n",
    "        return out_dict\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Kicks off train cycle and writes model parameter file at the end\n",
    "        \"\"\"\n",
    "        self._time_start = time.time()\n",
    "\n",
    "        print(\"Experiment started.\")\n",
    "\n",
    "        # Iterate over epochs\n",
    "        for self.epoch in range(self.n_epochs):\n",
    "            self.train()\n",
    "            self.validate()\n",
    "\n",
    "        # save model for inferencing\n",
    "        self.save_model_parameters()\n",
    "\n",
    "        self._time_end = time.time()\n",
    "        print(f\"Run complete. Total time: {time.strftime('%H:%M:%S', time.gmtime(self._time_end - self._time_start))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processed 260 files, total 9198 slices\n",
      "Experiment started.\n",
      "Training epoch 0...\n",
      "\n",
      "Epoch: 0 Train loss: 1.0856724977493286, 0.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.6045093536376953, 1.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.575187623500824, 2.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.567802369594574, 3.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.564964234828949, 5.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.568565309047699, 6.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.567985475063324, 7.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.553459107875824, 8.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.569023072719574, 10.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.574790894985199, 11.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.569877564907074, 12.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.561027467250824, 13.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.579002320766449, 15.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.593955934047699, 16.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.563224732875824, 17.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.580619752407074, 18.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.567619264125824, 20.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.581626832485199, 21.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.562400758266449, 22.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.571891725063324, 23.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.573539674282074, 25.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.575431764125824, 26.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.580223023891449, 27.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.573844850063324, 28.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.584861695766449, 29.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.565421998500824, 31.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.582511842250824, 32.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.587852418422699, 33.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.575828492641449, 34.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.564689576625824, 36.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.569267213344574, 37.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.572868287563324, 38.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.567619264125824, 39.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.574272096157074, 41.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.574912965297699, 42.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.564933717250824, 43.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.574851930141449, 44.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.569175660610199, 46.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.577934205532074, 47.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.575797975063324, 48.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.565025269985199, 49.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.565727174282074, 51.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.578361451625824, 52.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.572502076625824, 53.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.579307496547699, 54.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.567619264125824, 56.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.567649781703949, 57.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.592979371547699, 58.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.582084596157074, 59.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.565360963344574, 61.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.560295045375824, 62.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.570091187953949, 63.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.561973512172699, 64.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.594840943813324, 66.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.591728150844574, 67.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.585349977016449, 68.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.584617555141449, 69.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.569633424282074, 70.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.565849244594574, 72.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.584464967250824, 73.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.568260133266449, 74.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.583213746547699, 75.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.575767457485199, 77.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.571739137172699, 78.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.560905396938324, 79.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.568351686000824, 80.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.580924928188324, 82.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.570945680141449, 83.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.588645875453949, 84.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.570640504360199, 85.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.580314576625824, 87.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.571250855922699, 88.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.567130982875824, 89.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.564231812953949, 90.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.564506471157074, 92.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.579917848110199, 93.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.572135865688324, 94.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.587303102016449, 95.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.569145143032074, 97.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.568046510219574, 98.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.587638795375824, 99.5% complete\n",
      ".....\n",
      "Training complete\n",
      "Validating epoch 0...\n",
      "Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.568015992641449\n",
      "Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.555473268032074\n",
      "Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572898805141449\n",
      "Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.574882447719574\n",
      "Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.583671510219574\n",
      "Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.585838258266449\n",
      "Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571372926235199\n",
      "Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573020875453949\n",
      "Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578636109828949\n",
      "Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571861207485199\n",
      "Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571220338344574\n",
      "Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572532594203949\n",
      "Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570091187953949\n",
      "Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571678102016449\n",
      "Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579185426235199\n",
      "Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.568687379360199\n",
      "Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572685182094574\n",
      "Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580833375453949\n",
      "Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.588767945766449\n",
      "Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.587119996547699\n",
      "Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.585044801235199\n",
      "Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578941285610199\n",
      "Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577873170375824\n",
      "Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572227418422699\n",
      "Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580131471157074\n",
      "Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569389283657074\n",
      "Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579917848110199\n",
      "Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.568473756313324\n",
      "Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570945680141449\n",
      "Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.590873658657074\n",
      "Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.601219117641449\n",
      "Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577598512172699\n",
      "Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.560783326625824\n",
      "Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580314576625824\n",
      "Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.588920533657074\n",
      "Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578941285610199\n",
      "Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566184937953949\n",
      "Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566154420375824\n",
      "Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569084107875824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577476441860199\n",
      "Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.564964234828949\n",
      "Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572227418422699\n",
      "Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.564933717250824\n",
      "Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578483521938324\n",
      "Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.565055787563324\n",
      "Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.587943971157074\n",
      "Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.593009889125824\n",
      "Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.584190309047699\n",
      "Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.568107545375824\n",
      "Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.568077027797699\n",
      "Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577842652797699\n",
      "Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575431764125824\n",
      "Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567344605922699\n",
      "Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572532594203949\n",
      "Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570548951625824\n",
      "Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566215455532074\n",
      "Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580894410610199\n",
      "Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.559165894985199\n",
      "Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573661744594574\n",
      "Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575828492641449\n",
      "Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577751100063324\n",
      "Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.561759889125824\n",
      "Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582237184047699\n",
      "Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.586753785610199\n",
      "Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571983277797699\n",
      "Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.592216432094574\n",
      "Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566093385219574\n",
      "Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573265016078949\n",
      "Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577018678188324\n",
      "Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569847047328949\n",
      "Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.558463990688324\n",
      "Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577323853969574\n",
      "Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572135865688324\n",
      "Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579276978969574\n",
      "Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578666627407074\n",
      "Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576835572719574\n",
      "Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575797975063324\n",
      "Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.587669312953949\n",
      "Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.600181519985199\n",
      "Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.584526002407074\n",
      "Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577384889125824\n",
      "Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577415406703949\n",
      "Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566276490688324\n",
      "Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569541871547699\n",
      "Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572380006313324\n",
      "Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573600709438324\n",
      "Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573081910610199\n",
      "Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569389283657074\n",
      "Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.588066041469574\n",
      "Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.564781129360199\n",
      "Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.584037721157074\n",
      "Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573936402797699\n",
      "Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582481324672699\n",
      "Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.555046021938324\n",
      "Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.557304322719574\n",
      "Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.568351686000824\n",
      "Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.583610475063324\n",
      "Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.553001344203949\n",
      "Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571861207485199\n",
      "Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.564140260219574\n",
      "Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.551811158657074\n",
      "Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572257936000824\n",
      "Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566184937953949\n",
      "Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.557731568813324\n",
      "Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577293336391449\n",
      "Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580924928188324\n",
      "Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.561820924282074\n",
      "Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578605592250824\n",
      "Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577354371547699\n",
      "Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579948365688324\n",
      "Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570671021938324\n",
      "Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.558952271938324\n",
      "Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.584464967250824\n",
      "Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.563285768032074\n",
      "Batch 114. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569480836391449\n",
      "Batch 115. Data shape torch.Size([8, 1, 64, 64]) Loss 0.565177857875824\n",
      "Batch 116. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572288453578949\n",
      "Batch 117. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575462281703949\n",
      "Batch 118. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570274293422699\n",
      "Batch 119. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571311891078949\n",
      "Batch 120. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573081910610199\n",
      "Batch 121. Data shape torch.Size([8, 1, 64, 64]) Loss 0.562950074672699\n",
      "Batch 122. Data shape torch.Size([8, 1, 64, 64]) Loss 0.563896119594574\n",
      "Batch 123. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567619264125824\n",
      "Batch 124. Data shape torch.Size([8, 1, 64, 64]) Loss 0.585685670375824\n",
      "Batch 125. Data shape torch.Size([8, 1, 64, 64]) Loss 0.565452516078949\n",
      "Batch 126. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580161988735199\n",
      "Batch 127. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580375611782074\n",
      "Batch 128. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570762574672699\n",
      "Batch 129. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582786500453949\n",
      "Batch 130. Data shape torch.Size([8, 1, 64, 64]) Loss 0.581443727016449\n",
      "Batch 131. Data shape torch.Size([8, 1, 64, 64]) Loss 0.585288941860199\n",
      "Batch 132. Data shape torch.Size([8, 1, 64, 64]) Loss 0.592155396938324\n",
      "Batch 133. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573020875453949\n",
      "Batch 134. Data shape torch.Size([8, 1, 64, 64]) Loss 0.597770631313324\n",
      "Batch 135. Data shape torch.Size([8, 1, 64, 64]) Loss 0.553153932094574\n",
      "Batch 136. Data shape torch.Size([8, 1, 64, 64]) Loss 0.587760865688324\n",
      "Batch 137. Data shape torch.Size([8, 1, 64, 64]) Loss 0.594016969203949\n",
      "Batch 138. Data shape torch.Size([8, 1, 64, 64]) Loss 0.574790894985199\n",
      "Batch 139. Data shape torch.Size([8, 1, 64, 64]) Loss 0.583610475063324\n",
      "Batch 140. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571006715297699\n",
      "Batch 141. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578117311000824\n",
      "Batch 142. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567924439907074\n",
      "Batch 143. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572959840297699\n",
      "Batch 144. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575157105922699\n",
      "Batch 145. Data shape torch.Size([8, 1, 64, 64]) Loss 0.583976686000824\n",
      "Batch 146. Data shape torch.Size([8, 1, 64, 64]) Loss 0.589439332485199\n",
      "Batch 147. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569694459438324\n",
      "Batch 148. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567161500453949\n",
      "Batch 149. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582847535610199\n",
      "Batch 150. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582572877407074\n",
      "Batch 151. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566917359828949\n",
      "Batch 152. Data shape torch.Size([8, 1, 64, 64]) Loss 0.568046510219574\n",
      "Batch 153. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572685182094574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 154. Data shape torch.Size([8, 1, 64, 64]) Loss 0.574363648891449\n",
      "Batch 155. Data shape torch.Size([8, 1, 64, 64]) Loss 0.556938111782074\n",
      "Batch 156. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577354371547699\n",
      "Batch 157. Data shape torch.Size([8, 1, 64, 64]) Loss 0.597709596157074\n",
      "Batch 158. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582633912563324\n",
      "Batch 159. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569145143032074\n",
      "Batch 160. Data shape torch.Size([8, 1, 64, 64]) Loss 0.564781129360199\n",
      "Batch 161. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579215943813324\n",
      "Batch 162. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567802369594574\n",
      "Batch 163. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576896607875824\n",
      "Batch 164. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571586549282074\n",
      "Batch 165. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577384889125824\n",
      "Batch 166. Data shape torch.Size([8, 1, 64, 64]) Loss 0.563774049282074\n",
      "Batch 167. Data shape torch.Size([8, 1, 64, 64]) Loss 0.588371217250824\n",
      "Batch 168. Data shape torch.Size([8, 1, 64, 64]) Loss 0.581108033657074\n",
      "Batch 169. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582511842250824\n",
      "Batch 170. Data shape torch.Size([8, 1, 64, 64]) Loss 0.586143434047699\n",
      "Batch 171. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579734742641449\n",
      "Batch 172. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570518434047699\n",
      "Batch 173. Data shape torch.Size([8, 1, 64, 64]) Loss 0.585502564907074\n",
      "Batch 174. Data shape torch.Size([3, 1, 64, 64]) Loss 0.5720341205596924\n",
      "Validation complete\n",
      "Training epoch 1...\n",
      "\n",
      "Epoch: 1 Train loss: 0.571281373500824, 0.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.573936402797699, 1.4% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.570121705532074, 2.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.593803346157074, 3.9% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.565757691860199, 5.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.575736939907074, 6.3% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.571159303188324, 7.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.571861207485199, 8.8% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.566551148891449, 10.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.571220338344574, 11.3% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.576499879360199, 12.5% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.577262818813324, 13.8% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.561698853969574, 15.0% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.586112916469574, 16.3% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.570732057094574, 17.5% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.571464478969574, 18.8% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.562736451625824, 20.0% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.585197389125824, 21.2% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.570518434047699, 22.5% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.578666627407074, 23.7% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.566307008266449, 25.0% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.581565797328949, 26.2% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.570091187953949, 27.5% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.570671021938324, 28.7% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.592185914516449, 29.9% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.564567506313324, 31.2% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.566001832485199, 32.4% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.567405641078949, 33.7% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.566734254360199, 34.9% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.566245973110199, 36.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.572807252407074, 37.4% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.561027467250824, 38.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.571983277797699, 39.9% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.581809937953949, 41.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.587699830532074, 42.4% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.563926637172699, 43.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.581657350063324, 44.8% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.582328736782074, 46.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.561149537563324, 47.3% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.588798463344574, 48.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.584739625453949, 49.8% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.584861695766449, 51.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.571647584438324, 52.3% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.582115113735199, 53.5% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.559837281703949, 54.8% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.571525514125824, 56.0% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.559349000453949, 57.3% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.576377809047699, 58.5% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.559684693813324, 59.8% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.584922730922699, 61.0% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.569663941860199, 62.2% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.589225709438324, 63.5% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.568809449672699, 64.7% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.576469361782074, 66.0% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.582969605922699, 67.2% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.576530396938324, 68.4% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.586265504360199, 69.7% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.581321656703949, 70.9% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.562980592250824, 72.2% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.571983277797699, 73.4% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.578819215297699, 74.7% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.577110230922699, 75.9% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.579490602016449, 77.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.564079225063324, 78.4% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.572715699672699, 79.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.571372926235199, 80.9% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.568046510219574, 82.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.568138062953949, 83.4% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.575096070766449, 84.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.568656861782074, 85.8% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.572776734828949, 87.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.557762086391449, 88.3% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.590476930141449, 89.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.571708619594574, 90.8% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.576072633266449, 92.0% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.570609986782074, 93.3% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.581657350063324, 94.5% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.578544557094574, 95.8% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.580894410610199, 97.0% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.581718385219574, 98.3% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.561698853969574, 99.5% complete\n",
      ".....\n",
      "Training complete\n",
      "Validating epoch 1...\n",
      "Batch 0. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570640504360199\n",
      "Batch 1. Data shape torch.Size([8, 1, 64, 64]) Loss 0.601066529750824\n",
      "Batch 2. Data shape torch.Size([8, 1, 64, 64]) Loss 0.581321656703949\n",
      "Batch 3. Data shape torch.Size([8, 1, 64, 64]) Loss 0.596214234828949\n",
      "Batch 4. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567344605922699\n",
      "Batch 5. Data shape torch.Size([8, 1, 64, 64]) Loss 0.581718385219574\n",
      "Batch 6. Data shape torch.Size([8, 1, 64, 64]) Loss 0.562583863735199\n",
      "Batch 7. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575675904750824\n",
      "Batch 8. Data shape torch.Size([8, 1, 64, 64]) Loss 0.585380494594574\n",
      "Batch 9. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566520631313324\n",
      "Batch 10. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578269898891449\n",
      "Batch 11. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569969117641449\n",
      "Batch 12. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567100465297699\n",
      "Batch 13. Data shape torch.Size([8, 1, 64, 64]) Loss 0.553489625453949\n",
      "Batch 14. Data shape torch.Size([8, 1, 64, 64]) Loss 0.558402955532074\n",
      "Batch 15. Data shape torch.Size([8, 1, 64, 64]) Loss 0.559318482875824\n",
      "Batch 16. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567436158657074\n",
      "Batch 17. Data shape torch.Size([8, 1, 64, 64]) Loss 0.562156617641449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18. Data shape torch.Size([8, 1, 64, 64]) Loss 0.574607789516449\n",
      "Batch 19. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573081910610199\n",
      "Batch 20. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578544557094574\n",
      "Batch 21. Data shape torch.Size([8, 1, 64, 64]) Loss 0.585472047328949\n",
      "Batch 22. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576408326625824\n",
      "Batch 23. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579795777797699\n",
      "Batch 24. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576103150844574\n",
      "Batch 25. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573539674282074\n",
      "Batch 26. Data shape torch.Size([8, 1, 64, 64]) Loss 0.585655152797699\n",
      "Batch 27. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576286256313324\n",
      "Batch 28. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567436158657074\n",
      "Batch 29. Data shape torch.Size([8, 1, 64, 64]) Loss 0.574333131313324\n",
      "Batch 30. Data shape torch.Size([8, 1, 64, 64]) Loss 0.590293824672699\n",
      "Batch 31. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577079713344574\n",
      "Batch 32. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570640504360199\n",
      "Batch 33. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575309693813324\n",
      "Batch 34. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577781617641449\n",
      "Batch 35. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579856812953949\n",
      "Batch 36. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576927125453949\n",
      "Batch 37. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577445924282074\n",
      "Batch 38. Data shape torch.Size([8, 1, 64, 64]) Loss 0.586631715297699\n",
      "Batch 39. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569694459438324\n",
      "Batch 40. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571769654750824\n",
      "Batch 41. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569511353969574\n",
      "Batch 42. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570396363735199\n",
      "Batch 43. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572593629360199\n",
      "Batch 44. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576408326625824\n",
      "Batch 45. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580436646938324\n",
      "Batch 46. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578391969203949\n",
      "Batch 47. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572685182094574\n",
      "Batch 48. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579582154750824\n",
      "Batch 49. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572318971157074\n",
      "Batch 50. Data shape torch.Size([8, 1, 64, 64]) Loss 0.583885133266449\n",
      "Batch 51. Data shape torch.Size([8, 1, 64, 64]) Loss 0.595176637172699\n",
      "Batch 52. Data shape torch.Size([8, 1, 64, 64]) Loss 0.563590943813324\n",
      "Batch 53. Data shape torch.Size([8, 1, 64, 64]) Loss 0.591056764125824\n",
      "Batch 54. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567375123500824\n",
      "Batch 55. Data shape torch.Size([8, 1, 64, 64]) Loss 0.561088502407074\n",
      "Batch 56. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579795777797699\n",
      "Batch 57. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571464478969574\n",
      "Batch 58. Data shape torch.Size([8, 1, 64, 64]) Loss 0.560874879360199\n",
      "Batch 59. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579307496547699\n",
      "Batch 60. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575126588344574\n",
      "Batch 61. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572257936000824\n",
      "Batch 62. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576744019985199\n",
      "Batch 63. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580589234828949\n",
      "Batch 64. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566093385219574\n",
      "Batch 65. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573844850063324\n",
      "Batch 66. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573051393032074\n",
      "Batch 67. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573173463344574\n",
      "Batch 68. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571952760219574\n",
      "Batch 69. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569786012172699\n",
      "Batch 70. Data shape torch.Size([8, 1, 64, 64]) Loss 0.565086305141449\n",
      "Batch 71. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582084596157074\n",
      "Batch 72. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567161500453949\n",
      "Batch 73. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582969605922699\n",
      "Batch 74. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569816529750824\n",
      "Batch 75. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566795289516449\n",
      "Batch 76. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566581666469574\n",
      "Batch 77. Data shape torch.Size([8, 1, 64, 64]) Loss 0.597526490688324\n",
      "Batch 78. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571128785610199\n",
      "Batch 79. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566551148891449\n",
      "Batch 80. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571769654750824\n",
      "Batch 81. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567222535610199\n",
      "Batch 82. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582908570766449\n",
      "Batch 83. Data shape torch.Size([8, 1, 64, 64]) Loss 0.597343385219574\n",
      "Batch 84. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570487916469574\n",
      "Batch 85. Data shape torch.Size([8, 1, 64, 64]) Loss 0.557426393032074\n",
      "Batch 86. Data shape torch.Size([8, 1, 64, 64]) Loss 0.555076539516449\n",
      "Batch 87. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572898805141449\n",
      "Batch 88. Data shape torch.Size([8, 1, 64, 64]) Loss 0.562431275844574\n",
      "Batch 89. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582939088344574\n",
      "Batch 90. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573203980922699\n",
      "Batch 91. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575370728969574\n",
      "Batch 92. Data shape torch.Size([8, 1, 64, 64]) Loss 0.581474244594574\n",
      "Batch 93. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571861207485199\n",
      "Batch 94. Data shape torch.Size([8, 1, 64, 64]) Loss 0.574790894985199\n",
      "Batch 95. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578819215297699\n",
      "Batch 96. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572929322719574\n",
      "Batch 97. Data shape torch.Size([8, 1, 64, 64]) Loss 0.560478150844574\n",
      "Batch 98. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573112428188324\n",
      "Batch 99. Data shape torch.Size([8, 1, 64, 64]) Loss 0.590537965297699\n",
      "Batch 100. Data shape torch.Size([8, 1, 64, 64]) Loss 0.583976686000824\n",
      "Batch 101. Data shape torch.Size([8, 1, 64, 64]) Loss 0.581687867641449\n",
      "Batch 102. Data shape torch.Size([8, 1, 64, 64]) Loss 0.598808228969574\n",
      "Batch 103. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571952760219574\n",
      "Batch 104. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570426881313324\n",
      "Batch 105. Data shape torch.Size([8, 1, 64, 64]) Loss 0.581443727016449\n",
      "Batch 106. Data shape torch.Size([8, 1, 64, 64]) Loss 0.561180055141449\n",
      "Batch 107. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566337525844574\n",
      "Batch 108. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567253053188324\n",
      "Batch 109. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567436158657074\n",
      "Batch 110. Data shape torch.Size([8, 1, 64, 64]) Loss 0.562553346157074\n",
      "Batch 111. Data shape torch.Size([8, 1, 64, 64]) Loss 0.587730348110199\n",
      "Batch 112. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573570191860199\n",
      "Batch 113. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570518434047699\n",
      "Batch 114. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575065553188324\n",
      "Batch 115. Data shape torch.Size([8, 1, 64, 64]) Loss 0.562889039516449\n",
      "Batch 116. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573509156703949\n",
      "Batch 117. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577079713344574\n",
      "Batch 118. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580680787563324\n",
      "Batch 119. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577995240688324\n",
      "Batch 120. Data shape torch.Size([8, 1, 64, 64]) Loss 0.568138062953949\n",
      "Batch 121. Data shape torch.Size([8, 1, 64, 64]) Loss 0.571159303188324\n",
      "Batch 122. Data shape torch.Size([8, 1, 64, 64]) Loss 0.586051881313324\n",
      "Batch 123. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579856812953949\n",
      "Batch 124. Data shape torch.Size([8, 1, 64, 64]) Loss 0.584190309047699\n",
      "Batch 125. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582755982875824\n",
      "Batch 126. Data shape torch.Size([8, 1, 64, 64]) Loss 0.555961549282074\n",
      "Batch 127. Data shape torch.Size([8, 1, 64, 64]) Loss 0.590354859828949\n",
      "Batch 128. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576499879360199\n",
      "Batch 129. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580223023891449\n",
      "Batch 130. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572654664516449\n",
      "Batch 131. Data shape torch.Size([8, 1, 64, 64]) Loss 0.562614381313324\n",
      "Batch 132. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576652467250824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 133. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582237184047699\n",
      "Batch 134. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575645387172699\n",
      "Batch 135. Data shape torch.Size([8, 1, 64, 64]) Loss 0.577415406703949\n",
      "Batch 136. Data shape torch.Size([8, 1, 64, 64]) Loss 0.590049684047699\n",
      "Batch 137. Data shape torch.Size([8, 1, 64, 64]) Loss 0.561729371547699\n",
      "Batch 138. Data shape torch.Size([8, 1, 64, 64]) Loss 0.585594117641449\n",
      "Batch 139. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576499879360199\n",
      "Batch 140. Data shape torch.Size([8, 1, 64, 64]) Loss 0.574424684047699\n",
      "Batch 141. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576591432094574\n",
      "Batch 142. Data shape torch.Size([8, 1, 64, 64]) Loss 0.573173463344574\n",
      "Batch 143. Data shape torch.Size([8, 1, 64, 64]) Loss 0.599906861782074\n",
      "Batch 144. Data shape torch.Size([8, 1, 64, 64]) Loss 0.561942994594574\n",
      "Batch 145. Data shape torch.Size([8, 1, 64, 64]) Loss 0.581687867641449\n",
      "Batch 146. Data shape torch.Size([8, 1, 64, 64]) Loss 0.556877076625824\n",
      "Batch 147. Data shape torch.Size([8, 1, 64, 64]) Loss 0.584831178188324\n",
      "Batch 148. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570243775844574\n",
      "Batch 149. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580894410610199\n",
      "Batch 150. Data shape torch.Size([8, 1, 64, 64]) Loss 0.566673219203949\n",
      "Batch 151. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576957643032074\n",
      "Batch 152. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572837769985199\n",
      "Batch 153. Data shape torch.Size([8, 1, 64, 64]) Loss 0.570426881313324\n",
      "Batch 154. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567863404750824\n",
      "Batch 155. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567527711391449\n",
      "Batch 156. Data shape torch.Size([8, 1, 64, 64]) Loss 0.560874879360199\n",
      "Batch 157. Data shape torch.Size([8, 1, 64, 64]) Loss 0.572257936000824\n",
      "Batch 158. Data shape torch.Size([8, 1, 64, 64]) Loss 0.583702027797699\n",
      "Batch 159. Data shape torch.Size([8, 1, 64, 64]) Loss 0.568046510219574\n",
      "Batch 160. Data shape torch.Size([8, 1, 64, 64]) Loss 0.574974000453949\n",
      "Batch 161. Data shape torch.Size([8, 1, 64, 64]) Loss 0.576255738735199\n",
      "Batch 162. Data shape torch.Size([8, 1, 64, 64]) Loss 0.578666627407074\n",
      "Batch 163. Data shape torch.Size([8, 1, 64, 64]) Loss 0.569114625453949\n",
      "Batch 164. Data shape torch.Size([8, 1, 64, 64]) Loss 0.575797975063324\n",
      "Batch 165. Data shape torch.Size([8, 1, 64, 64]) Loss 0.580406129360199\n",
      "Batch 166. Data shape torch.Size([8, 1, 64, 64]) Loss 0.579215943813324\n",
      "Batch 167. Data shape torch.Size([8, 1, 64, 64]) Loss 0.574699342250824\n",
      "Batch 168. Data shape torch.Size([8, 1, 64, 64]) Loss 0.583396852016449\n",
      "Batch 169. Data shape torch.Size([8, 1, 64, 64]) Loss 0.582145631313324\n",
      "Batch 170. Data shape torch.Size([8, 1, 64, 64]) Loss 0.565055787563324\n",
      "Batch 171. Data shape torch.Size([8, 1, 64, 64]) Loss 0.567619264125824\n",
      "Batch 172. Data shape torch.Size([8, 1, 64, 64]) Loss 0.584312379360199\n",
      "Batch 173. Data shape torch.Size([8, 1, 64, 64]) Loss 0.591636598110199\n",
      "Batch 174. Data shape torch.Size([3, 1, 64, 64]) Loss 0.5665002465248108\n",
      "Validation complete\n",
      "Training epoch 2...\n",
      "\n",
      "Epoch: 2 Train loss: 0.576560914516449, 0.1% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.571952760219574, 1.4% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.575004518032074, 2.6% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.575797975063324, 3.9% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.577629029750824, 5.1% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.573051393032074, 6.3% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.559318482875824, 7.6% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.564292848110199, 8.8% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.575431764125824, 10.1% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.576225221157074, 11.3% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.564414918422699, 12.5% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.580680787563324, 13.8% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.587150514125824, 15.0% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.576042115688324, 16.3% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.563285768032074, 17.5% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.577415406703949, 18.8% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.570365846157074, 20.0% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.575157105922699, 21.2% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.590354859828949, 22.5% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.581993043422699, 23.7% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.569053590297699, 25.0% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.575004518032074, 26.2% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.592063844203949, 27.5% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.577354371547699, 28.7% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.579490602016449, 29.9% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.572349488735199, 31.2% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.566276490688324, 32.4% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.596794068813324, 33.7% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.575584352016449, 34.9% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.571006715297699, 36.1% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.580894410610199, 37.4% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.582542359828949, 38.6% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.575675904750824, 39.9% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.573570191860199, 41.1% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.577354371547699, 42.4% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.583610475063324, 43.6% complete\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Holds configuration parameters\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"Basic_unet\"\n",
    "        self.root_dir = r\"../data\"\n",
    "        self.n_epochs = 10\n",
    "        self.learning_rate = 0.0002\n",
    "        self.batch_size = 8\n",
    "        self.patch_size = 64\n",
    "        self.test_results_dir = \"test_results\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get configuration\n",
    "    c = Config()\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    data = LoadHippocampusData(c.root_dir, y_shape = c.patch_size, z_shape = c.patch_size)\n",
    "\n",
    "    # create three keys in the dictionary (\"train\", \"val\" and \"test\") with each key storing the array with indices \n",
    "    split = dict()\n",
    "    n = len(data)\n",
    "    idx_list = list(range(n))\n",
    "    random.shuffle(idx_list)\n",
    "    split['train'] = idx_list[ :int(n * 0.7)]\n",
    "    split['val'] = idx_list[int(n * 0.7) : int(n * 0.85)]\n",
    "    split['test'] = idx_list[int(n * 0.85):]  \n",
    "\n",
    "    # Set up and run experiment\n",
    "    exp = UNetExperiment(c, split, data)\n",
    "\n",
    "    # run training\n",
    "    exp.run()\n",
    "\n",
    "    # prep and run testing\n",
    "    results_json = exp.run_test()\n",
    "\n",
    "    results_json[\"config\"] = vars(c)\n",
    "\n",
    "    with open(os.path.join(exp.out_dir, \"results.json\"), 'w') as out_file:\n",
    "        json.dump(results_json, out_file, indent=2, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in exp.train_loader:\n",
    "    y = np.squeeze(x['image'][0, :, :])\n",
    "    print(y.max())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.model.eval()\n",
    "\n",
    "# volume is a numpy array of shape [X,Y,Z] and I will slice X axis\n",
    "slices = []\n",
    "\n",
    "# create mask for each slice across the X (0th) dimension. \n",
    "# put all slices into a 3D Numpy array\n",
    "# volume = exp.test_data[0]['image']\n",
    "volume = exp.test_data[0]['image']\n",
    "img = volume[0,:,:]\n",
    "print('img', img.shape, img)\n",
    "z = np.squeeze(img)\n",
    "print(z.shape)\n",
    "print(z.max())\n",
    "# slc = img.astype(np.single)/np.max(img)\n",
    "# print('slc', slc.shape, slc)\n",
    "# slc_tensor = torch.from_numpy(slc).unsqueeze(0).unsqueeze(0).to(exp.device)\n",
    "# print('slc_tensor', slc_tensor.shape, slc_tensor)\n",
    "slc_tensor = torch.from_numpy(volume[0,:,:].astype(np.single)).unsqueeze(0).unsqueeze(0).to(exp.device)\n",
    "pred = exp.model(slc_tensor)\n",
    "print('pred', pred.shape, pred)\n",
    "mask = torch.argmax(np.squeeze(pred.cpu().detach()), dim=0)\n",
    "print('mask', mask.shape, mask)\n",
    "print((mask>0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = exp.test_data[0]['seg']\n",
    "for x in exp.test_data:\n",
    "    print((x['seg']>0).sum() / (x['seg']==0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
